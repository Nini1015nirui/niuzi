#!/usr/bin/env python3
"""
Dataset Format Converter for Spatial-Mamba Segmentation
æ•°æ®é›†æ ¼å¼è½¬æ¢å·¥å…·

æ”¯æŒçš„è½¬æ¢æ ¼å¼ï¼š
1. COCOæ ¼å¼ -> ADE20Kæ ¼å¼
2. VOCæ ¼å¼ -> ADE20Kæ ¼å¼  
3. å•ç‹¬çš„RGBå½©è‰²æ ‡æ³¨ -> ç°åº¦ç´¢å¼•æ ‡æ³¨
4. å…¶ä»–å¸¸è§æ ¼å¼
"""

import os
import cv2
import json
import numpy as np
from pathlib import Path
import argparse
from PIL import Image
import shutil
from tqdm import tqdm


class DatasetConverter:
    """æ•°æ®é›†æ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self, input_dir, output_dir):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        for split in ['training', 'validation']:
            (self.output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)
            (self.output_dir / 'annotations' / split).mkdir(parents=True, exist_ok=True)
    
    def convert_voc_format(self, train_ratio=0.8):
        """
        è½¬æ¢VOCæ ¼å¼æ•°æ®é›†
        
        æœŸæœ›è¾“å…¥ç»“æ„ï¼š
        input_dir/
        â”œâ”€â”€ JPEGImages/     # åŸå§‹å›¾åƒ
        â”œâ”€â”€ SegmentationClass/  # åˆ†å‰²æ ‡æ³¨
        â””â”€â”€ ImageSets/
            â””â”€â”€ Segmentation/
                â”œâ”€â”€ train.txt
                â””â”€â”€ val.txt
        """
        print("ğŸ”„ è½¬æ¢VOCæ ¼å¼æ•°æ®é›†...")
        
        jpeg_dir = self.input_dir / 'JPEGImages'
        seg_dir = self.input_dir / 'SegmentationClass' 
        sets_dir = self.input_dir / 'ImageSets' / 'Segmentation'
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not all([jpeg_dir.exists(), seg_dir.exists()]):
            print("âŒ VOCæ ¼å¼ç›®å½•ä¸å®Œæ•´")
            return False
        
        # è¯»å–åˆ†å‰²åˆ—è¡¨
        train_list = []
        val_list = []
        
        if (sets_dir / 'train.txt').exists():
            with open(sets_dir / 'train.txt', 'r') as f:
                train_list = [line.strip() for line in f.readlines()]
        
        if (sets_dir / 'val.txt').exists():
            with open(sets_dir / 'val.txt', 'r') as f:
                val_list = [line.strip() for line in f.readlines()]
        
        # å¦‚æœæ²¡æœ‰åˆ†å‰²åˆ—è¡¨ï¼Œè‡ªåŠ¨åˆ›å»º
        if not train_list and not val_list:
            all_images = [f.stem for f in jpeg_dir.glob('*.jpg')]
            split_idx = int(len(all_images) * train_ratio)
            train_list = all_images[:split_idx]
            val_list = all_images[split_idx:]
            print(f"ğŸ“‹ è‡ªåŠ¨åˆ†å‰²: è®­ç»ƒé›†{len(train_list)}å¼ , éªŒè¯é›†{len(val_list)}å¼ ")
        
        # è½¬æ¢è®­ç»ƒé›†
        self._convert_voc_split(jpeg_dir, seg_dir, train_list, 'training')
        
        # è½¬æ¢éªŒè¯é›†
        self._convert_voc_split(jpeg_dir, seg_dir, val_list, 'validation')
        
        print("âœ… VOCæ ¼å¼è½¬æ¢å®Œæˆ")
        return True
    
    def _convert_voc_split(self, jpeg_dir, seg_dir, file_list, split):
        """è½¬æ¢VOCå•ä¸ªåˆ†å‰²"""
        output_img_dir = self.output_dir / 'images' / split
        output_ann_dir = self.output_dir / 'annotations' / split
        
        for filename in tqdm(file_list, desc=f"è½¬æ¢{split}é›†"):
            # å¤åˆ¶å›¾åƒ
            img_src = jpeg_dir / f"{filename}.jpg"
            if not img_src.exists():
                img_src = jpeg_dir / f"{filename}.png"
            
            if img_src.exists():
                img_dst = output_img_dir / f"{filename}.jpg"
                shutil.copy2(img_src, img_dst)
            
            # è½¬æ¢æ ‡æ³¨
            ann_src = seg_dir / f"{filename}.png"
            if ann_src.exists():
                ann_dst = output_ann_dir / f"{filename}.png"
                self._convert_annotation(ann_src, ann_dst)
    
    def convert_rgb_annotations(self, class_colors, train_ratio=0.8):
        """
        è½¬æ¢RGBå½©è‰²æ ‡æ³¨ä¸ºç°åº¦ç´¢å¼•æ ‡æ³¨
        
        Args:
            class_colors: ç±»åˆ«é¢œè‰²æ˜ å°„ {class_id: [R, G, B]}
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        """
        print("ğŸ”„ è½¬æ¢RGBå½©è‰²æ ‡æ³¨...")
        
        # å‡è®¾è¾“å…¥ç›®å½•åŒ…å«imageså’Œannotationså­ç›®å½•
        input_img_dir = self.input_dir / 'images'
        input_ann_dir = self.input_dir / 'annotations'
        
        if not input_img_dir.exists() or not input_ann_dir.exists():
            print("âŒ æ‰¾ä¸åˆ°imagesæˆ–annotationsç›®å½•")
            return False
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        img_files = list(input_img_dir.glob('*.jpg')) + list(input_img_dir.glob('*.png'))
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(img_files) * train_ratio)
        train_files = img_files[:split_idx]
        val_files = img_files[split_idx:]
        
        # è½¬æ¢è®­ç»ƒé›†
        self._convert_rgb_split(input_img_dir, input_ann_dir, train_files, 'training', class_colors)
        
        # è½¬æ¢éªŒè¯é›†
        self._convert_rgb_split(input_img_dir, input_ann_dir, val_files, 'validation', class_colors)
        
        print("âœ… RGBæ ‡æ³¨è½¬æ¢å®Œæˆ")
        return True
    
    def _convert_rgb_split(self, input_img_dir, input_ann_dir, file_list, split, class_colors):
        """è½¬æ¢RGBæ ‡æ³¨å•ä¸ªåˆ†å‰²"""
        output_img_dir = self.output_dir / 'images' / split
        output_ann_dir = self.output_dir / 'annotations' / split
        
        for img_file in tqdm(file_list, desc=f"è½¬æ¢{split}é›†"):
            # å¤åˆ¶å›¾åƒ
            img_dst = output_img_dir / img_file.name
            shutil.copy2(img_file, img_dst)
            
            # è½¬æ¢æ ‡æ³¨
            ann_file = input_ann_dir / (img_file.stem + '.png')
            if ann_file.exists():
                ann_dst = output_ann_dir / (img_file.stem + '.png')
                self._convert_rgb_to_index(ann_file, ann_dst, class_colors)
    
    def _convert_rgb_to_index(self, rgb_path, output_path, class_colors):
        """å°†RGBå½©è‰²æ ‡æ³¨è½¬æ¢ä¸ºç°åº¦ç´¢å¼•æ ‡æ³¨"""
        # è¯»å–RGBæ ‡æ³¨
        rgb_img = cv2.imread(str(rgb_path))
        rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
        
        # åˆ›å»ºç´¢å¼•æ ‡æ³¨
        h, w = rgb_img.shape[:2]
        index_img = np.zeros((h, w), dtype=np.uint8)
        
        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…åƒç´ 
        for class_id, color in class_colors.items():
            # åˆ›å»ºé¢œè‰²æ©ç 
            mask = np.all(rgb_img == color, axis=2)
            index_img[mask] = class_id
        
        # ä¿å­˜ç´¢å¼•æ ‡æ³¨
        cv2.imwrite(str(output_path), index_img)
    
    def _convert_annotation(self, src_path, dst_path):
        """è½¬æ¢å•ä¸ªæ ‡æ³¨æ–‡ä»¶ï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
        # è¯»å–æºæ ‡æ³¨
        ann = cv2.imread(str(src_path), cv2.IMREAD_GRAYSCALE)
        
        if ann is None:
            print(f"âš ï¸ æ— æ³•è¯»å–æ ‡æ³¨æ–‡ä»¶: {src_path}")
            return
        
        # ç›´æ¥ä¿å­˜ï¼ˆå¦‚æœå·²ç»æ˜¯ç´¢å¼•æ ¼å¼ï¼‰
        cv2.imwrite(str(dst_path), ann)
    
    def convert_coco_format(self, annotation_file, train_ratio=0.8):
        """
        è½¬æ¢COCOæ ¼å¼æ•°æ®é›†
        
        Args:
            annotation_file: COCOæ ‡æ³¨JSONæ–‡ä»¶è·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        """
        print("ğŸ”„ è½¬æ¢COCOæ ¼å¼æ•°æ®é›†...")
        
        with open(annotation_file, 'r') as f:
            coco_data = json.load(f)
        
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„COCOæ ¼å¼å®ç°è½¬æ¢é€»è¾‘
        # ç”±äºCOCOæ ¼å¼æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºæœ¬æ¡†æ¶
        print("âš ï¸ COCOæ ¼å¼è½¬æ¢éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µå®ç°")
        return False
    
    def create_data_info(self):
        """åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶"""
        print("ğŸ“Š ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯...")
        
        info = {
            'name': self.output_dir.name,
            'splits': {},
            'classes': set()
        }
        
        for split in ['training', 'validation']:
            ann_dir = self.output_dir / 'annotations' / split
            img_dir = self.output_dir / 'images' / split
            
            ann_files = list(ann_dir.glob('*.png'))
            img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))
            
            info['splits'][split] = {
                'num_images': len(img_files),
                'num_annotations': len(ann_files)
            }
            
            # ç»Ÿè®¡ç±»åˆ«
            for ann_file in ann_files:
                ann = cv2.imread(str(ann_file), cv2.IMREAD_GRAYSCALE)
                if ann is not None:
                    unique_classes = np.unique(ann)
                    info['classes'].update(unique_classes.tolist())
        
        # è½¬æ¢setä¸ºlist
        info['classes'] = sorted(list(info['classes']))
        info['num_classes'] = len(info['classes'])
        
        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        info_file = self.output_dir / 'dataset_info.json'
        with open(info_file, 'w') as f:
            json.dump(info, f, indent=2)
        
        print(f"ğŸ“„ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}")
        
        return info


def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†æ ¼å¼è½¬æ¢å·¥å…·')
    parser.add_argument('input_dir', help='è¾“å…¥æ•°æ®é›†ç›®å½•')
    parser.add_argument('output_dir', help='è¾“å‡ºæ•°æ®é›†ç›®å½•') 
    parser.add_argument('--format', choices=['voc', 'rgb', 'coco'], 
                       required=True, help='è¾“å…¥æ•°æ®é›†æ ¼å¼')
    parser.add_argument('--train-ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--class-colors', 
                       help='RGBæ ¼å¼çš„ç±»åˆ«é¢œè‰²æ˜ å°„JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    converter = DatasetConverter(args.input_dir, args.output_dir)
    
    success = False
    
    if args.format == 'voc':
        success = converter.convert_voc_format(args.train_ratio)
    
    elif args.format == 'rgb':
        if not args.class_colors:
            print("âŒ RGBæ ¼å¼éœ€è¦æä¾›--class-colorså‚æ•°")
            return
        
        with open(args.class_colors, 'r') as f:
            class_colors = json.load(f)
        
        # è½¬æ¢å­—ç¬¦ä¸²é”®ä¸ºæ•´æ•°
        class_colors = {int(k): v for k, v in class_colors.items()}
        
        success = converter.convert_rgb_annotations(class_colors, args.train_ratio)
    
    elif args.format == 'coco':
        print("âŒ COCOæ ¼å¼è½¬æ¢å°šæœªå®ç°")
        return
    
    if success:
        converter.create_data_info()
        print("\nğŸ‰ æ•°æ®é›†è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œæ•°æ®é›†éªŒè¯å·¥å…·")
        print("2. è°ƒæ•´è®­ç»ƒé…ç½®æ–‡ä»¶")
        print("3. å¼€å§‹è®­ç»ƒ")


if __name__ == '__main__':
    main()