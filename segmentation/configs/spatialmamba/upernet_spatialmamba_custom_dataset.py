# Custom Dataset Training Configuration for Spatial-Mamba
# åŸºäºä½ çš„è‡ªå®šä¹‰æ•°æ®é›†çš„Spatial-Mambaåˆ†å‰²è®­ç»ƒé…ç½®

_base_ = [
    '../_base_/models/upernet_r50.py',  # åŸºç¡€æ¨¡å‹é…ç½®
    '../_base_/datasets/custom_dataset.py',  # ä½ çš„æ•°æ®é›†é…ç½®
    '../_base_/default_runtime.py',    # è¿è¡Œæ—¶é…ç½®
    '../_base_/schedules/schedule_160k.py'  # è®­ç»ƒç­–ç•¥
]

# =====================================================
# æ¨¡å‹é…ç½® - Spatial-Mamba
# =====================================================
model = dict(
    backbone=dict(
        _delete_=True,  # åˆ é™¤åŸæœ‰backboneé…ç½®
        type='MM_SpatialMamba',
        out_indices=(0, 1, 2, 3),
        pretrained="",  # é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼Œç•™ç©ºè¡¨ç¤ºéšæœºåˆå§‹åŒ–
        
        # æ¨¡å‹å°ºå¯¸é…ç½®ï¼ˆå¯é€‰æ‹©ï¼štiny, small, baseï¼‰
        dims=64,        # tiny: 64, small: 96, base: 128
        d_state=1,      # MambaçŠ¶æ€ç»´åº¦
        depths=(2, 4, 8, 4),  # å„å±‚æ·±åº¦
        drop_path_rate=0.2,   # Drop pathæ¯”ç‡
    ),
    
    # è§£ç å¤´é…ç½® - éœ€è¦æ ¹æ®ä½ çš„ç±»åˆ«æ•°è°ƒæ•´
    decode_head=dict(
        in_channels=[64, 128, 256, 512],  # å¯¹åº”backboneè¾“å‡ºé€šé“
        num_classes=21,  # ğŸ”¥ å…³é”®ï¼šä¿®æ”¹ä¸ºä½ çš„ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
    ),
    
    auxiliary_head=dict(
        in_channels=256,
        num_classes=21,  # ğŸ”¥ å…³é”®ï¼šä¸decode_headä¿æŒä¸€è‡´
    )
)

# =====================================================
# æ•°æ®é›†é…ç½®è¦†ç›–
# =====================================================
# å¦‚æœä½ çš„æ•°æ®é›†è·¯å¾„ä¸åŒï¼Œåœ¨è¿™é‡Œè¦†ç›–
data_root = 'my_datasets/your_dataset_name'  # ğŸ”¥ ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„

train_dataloader = dict(
    batch_size=2,  # ğŸ”¥ æ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼ˆRTX 4060å»ºè®®2-4ï¼‰
    dataset=dict(
        data_root=data_root,
    ))

val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_root=data_root,
    ))

test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        data_root=data_root,
    ))

# =====================================================
# è®­ç»ƒé…ç½®è°ƒæ•´
# =====================================================
# è®­ç»ƒè¿­ä»£æ•°ï¼ˆæ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´ï¼‰
train_cfg = dict(
    max_iters=40000,    # ğŸ”¥ æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´
    val_interval=2000   # æ¯2000æ¬¡è¿­ä»£éªŒè¯ä¸€æ¬¡
)

# å­¦ä¹ ç‡ç­–ç•¥
param_scheduler = [
    dict(
        type='LinearLR', 
        start_factor=1e-6, 
        by_epoch=False, 
        begin=0, 
        end=1500),
    dict(
        type='PolyLR',
        eta_min=0.0,
        power=1.0,
        begin=1500,
        end=40000,  # ä¸max_itersä¿æŒä¸€è‡´
        by_epoch=False,
    )
]

# ä¼˜åŒ–å™¨é…ç½®
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(
        type='AdamW', 
        lr=0.00006,  # ğŸ”¥ å­¦ä¹ ç‡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
        betas=(0.9, 0.999), 
        weight_decay=0.01),
    paramwise_cfg=dict(
        custom_keys={
            'absolute_pos_embed': dict(decay_mult=0.),
            'relative_position_bias_table': dict(decay_mult=0.),
            'norm': dict(decay_mult=0.)
        })
)

# =====================================================
# æ—¥å¿—å’Œæ£€æŸ¥ç‚¹é…ç½®
# =====================================================
default_hooks = dict(
    checkpoint=dict(
        type='CheckpointHook', 
        by_epoch=False, 
        interval=2000,  # æ¯2000æ¬¡è¿­ä»£ä¿å­˜æ£€æŸ¥ç‚¹
        max_keep_ckpts=3),  # æœ€å¤šä¿ç•™3ä¸ªæ£€æŸ¥ç‚¹
    logger=dict(
        type='LoggerHook', 
        interval=50,  # æ¯50æ¬¡è¿­ä»£æ‰“å°æ—¥å¿—
        log_metric_by_epoch=False),
)

# å·¥ä½œç›®å½•
work_dir = './work_dirs/custom_dataset_training'

# =====================================================
# ä½¿ç”¨è¯´æ˜
# =====================================================
"""
ä½¿ç”¨æ­¤é…ç½®è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†çš„æ­¥éª¤ï¼š

1. å‡†å¤‡æ•°æ®é›†ï¼ˆæŒ‰ç…§æŒ‡å®šæ ¼å¼ï¼‰
2. ä¿®æ”¹å…³é”®å‚æ•°ï¼š
   - data_root: æ•°æ®é›†è·¯å¾„
   - num_classes: ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ï¼‰
   - max_iters: è®­ç»ƒè¿­ä»£æ•°
   - batch_size: æ‰¹é‡å¤§å°
   - lr: å­¦ä¹ ç‡

3. å¼€å§‹è®­ç»ƒï¼š
   cd segmentation
   python tools/train.py configs/spatialmamba/upernet_spatialmamba_custom_dataset.py

4. ç›‘æ§è®­ç»ƒï¼š
   tensorboard --logdir work_dirs/custom_dataset_training

5. æµ‹è¯•æ¨¡å‹ï¼š
   python tools/test.py configs/spatialmamba/upernet_spatialmamba_custom_dataset.py work_dirs/custom_dataset_training/latest.pth
"""